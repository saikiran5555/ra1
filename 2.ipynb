{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb11e77c",
   "metadata": {},
   "source": [
    "Linear regression is a statistical method used for modeling the relationship between a dependent variable and one or more independent variables. This model assumes a linear relationship between variables. However, to effectively apply linear regression, several key assumptions must be met. These assumptions are essential for the validity of the inference.\n",
    "\n",
    "Assumptions of Linear Regression:\n",
    "Linearity: The relationship between the independent variables and the dependent variable is linear. This can be checked by plotting scatter plots of the independent variables against the dependent variable and looking for linear patterns.\n",
    "\n",
    "Independence: Observations are independent of each other. This is crucial for the standard errors of the coefficients to be valid. Independence can be a challenge with time-series data where subsequent observations might be correlated. This assumption can be tested using Durbin-Watson test for autocorrelation.\n",
    "\n",
    "Homoscedasticity: This means that the variance of the residuals (the differences between the observed values and the values predicted by the model) is constant across all levels of the independent variable(s). If the variance is non-constant, the model is heteroscedastic. You can check this assumption by looking at a plot of the residuals versus the predicted values. A funnel-shaped pattern indicates heteroscedasticity.\n",
    "\n",
    "Normal Distribution of Errors: The regression model assumes that the residuals are normally distributed. This is particularly important for making inferences about the statistical significance of the coefficients. A Q-Q plot or a normality test like the Shapiro-Wilk test can be used to check this assumption.\n",
    "\n",
    "No or Minimal Multicollinearity: Multicollinearity occurs when independent variables in a regression model are highly correlated. This can make it difficult to determine the individual effect of each variable and can inflate the standard errors of the coefficients. Tools to detect multicollinearity include the Variance Inflation Factor (VIF) and correlation matrices.\n",
    "\n",
    "Checking the Assumptions:\n",
    "Linearity Check: Use scatter plots to visualize the relationship between each independent variable and the dependent variable. Also, you can include polynomial or interaction terms if the relationship appears non-linear.\n",
    "\n",
    "Independence Check: For time-series data, use the Durbin-Watson test. For cross-sectional data, this is more about study design and ensuring that the data collection method does not introduce dependency.\n",
    "\n",
    "Homoscedasticity Check: Plot the residuals against the predicted (fitted) values. The absence of patterns (like a funnel shape) generally indicates homoscedasticity.\n",
    "\n",
    "Normality of Errors Check: Create a Q-Q plot of the residuals. If the residuals fall along a straight line, they are normally distributed. Statistical tests like the Shapiro-Wilk test can also be used.\n",
    "\n",
    "Multicollinearity Check: Calculate the Variance Inflation Factor (VIF) for each predictor. A VIF value greater than 5 or 10 indicates high multicollinearity. Also, inspect correlation matrices to check for high correlations between predictors.\n",
    "\n",
    "Additional Considerations:\n",
    "Model Specification: Ensuring that the model includes all relevant variables and only relevant variables. Omitting a variable that affects the dependent variable or including one that does not can bias the estimates.\n",
    "\n",
    "Robustness Checks: It's often beneficial to run various forms of your model to see how sensitive your results are to different specifications.\n",
    "\n",
    "Outliers and Leverage Points: Outliers can have a large impact on the regression line. Use diagnostic plots to identify and address outliers.\n",
    "\n",
    "Data Transformation: Sometimes, transforming variables (like using log, square root, or reciprocal transformations) can help in meeting these assumptions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
